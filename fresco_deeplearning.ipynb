{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinnero/pinnero/blob/main/fresco_deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPfproSYUAKU",
        "outputId": "1ec466c7-c65c-492e-9aa5-a038e11737fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# Update the paths to your data in Google Drive\n",
        "drive_root = '/content/drive/MyDrive'  # Root directory of your Google Drive\n",
        "data_path = f\"{drive_root}/labeled_images\"  # Update this path\n",
        "\n",
        "# Load dataset and apply transformations\n",
        "dataset = ImageFolder(root=data_path, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for training and validation\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom CNN architecture with ResNet transfer learning\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        # Load a pre-trained ResNet model (ResNet18 in this case)\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Freeze all layers of ResNet to avoid training them\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "       # Replace the last fully connected layer of ResNet\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(in_features, num_classes)  # Change this line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the custom CNN model\n",
        "model = CustomCNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "eF_Qdutug0-U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lists to store accuracy values per epoch\n",
        "val_accuracy_history = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass and compute loss\n",
        "            outputs = model(images)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "            # Count correct predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100.0 * correct / len(val_dataset)\n",
        "\n",
        "    # Append validation accuracy to history\n",
        "    val_accuracy_history.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Validation Loss: {val_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot the validation accuracy trends\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "plt.plot(epochs, val_accuracy_history)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy (%)\")\n",
        "plt.title(\"Validation Accuracy per Epoch\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dQnHMjhphEz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823885d6-e608-4bd8-d461-9976e657e9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] | Validation Loss: 1.3839 | Accuracy: 30.51%\n",
            "Epoch [2/10] | Validation Loss: 1.3303 | Accuracy: 38.98%\n",
            "Epoch [3/10] | Validation Loss: 1.2856 | Accuracy: 40.68%\n",
            "Epoch [4/10] | Validation Loss: 1.1965 | Accuracy: 49.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_path = \"/path/to/your/test/dataset\"  # Replace with the path to your test dataset\n",
        "test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "\n",
        "# Create the test_loader using DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Training and validation loops as you provided before...\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # Training loop as before...\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Validation loop as before...\n",
        "\n",
        "    # Print validation results as before...\n",
        "\n",
        "    # Test loop\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass for test data\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Count correct predictions for accuracy calculation\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100.0 * correct_test / len(test_dataset)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Training and validation completed after all epochs...\n",
        "print(\"Training and Testing completed.\")\n"
      ],
      "metadata": {
        "id": "zIklLacMhj00"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}